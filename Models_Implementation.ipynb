{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hummna/Core-Human-Facial-Emotions-Detection./blob/main/Models_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc3e1c3"
      },
      "source": [
        "# Importing"
      ],
      "id": "8fc3e1c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a08122b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Dense,BatchNormalization, Flatten, MaxPool2D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from keras.layers import Conv2D, Reshape\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from keras.backend import epsilon\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import cv2\n",
        "\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "\n",
        "import os"
      ],
      "id": "2a08122b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C9_l5Q9pTqS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "7C9_l5Q9pTqS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd01f1e9"
      },
      "source": [
        "# Data Preprocessing"
      ],
      "id": "cd01f1e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "527d79db"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(os.listdir(\"/content/drive/MyDrive/Colab Notebooks/train\"))"
      ],
      "id": "527d79db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1df4238d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "angry= '/content/drive/MyDrive/Colab Notebooks/train/angry'\n",
        "disgust = '/content/drive/MyDrive/Colab Notebooks/train/disgust'\n",
        "fear = '/content/drive/MyDrive/Colab Notebooks/train/fear'\n",
        "neutral = '/content/drive/MyDrive/Colab Notebooks/train/neutral'\n",
        "surprise = '/content/drive/MyDrive/Colab Notebooks/train/surprise'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "angry_path = os.listdir(angry)\n",
        "disgust_path = os.listdir(disgust)\n",
        "fear_path = os.listdir(fear)\n",
        "neutral_path = os.listdir(neutral)\n",
        "surprise_path = os.listdir(surprise)"
      ],
      "id": "1df4238d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0afd7ba8"
      },
      "source": [
        "# Sample Images"
      ],
      "id": "0afd7ba8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5145fab8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_img(path):\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.resize(image,(224, 224))\n",
        "    return image[...,::-1]"
      ],
      "id": "5145fab8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16673568"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_img(path):\n",
        "    image = cv2.imread(path)\n",
        "    if image is not None:  # Check if image is loaded successfully\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        return image[...,::-1]\n",
        "    else:\n",
        "        print(f\"/content/drive/MyDrive/Colab Notebooks/train/angry: {path}\")\n",
        "        return None\n"
      ],
      "id": "16673568"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24ac0eb4"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    image = cv2.imread(path)\n",
        "    if image is not None:  # Check if image is loaded successfully\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        return image[...,::-1]\n",
        "    else:\n",
        "        print(f\"/content/drive/MyDrive/Colab Notebooks/train/disgust: {path}\")\n",
        "        return None\n"
      ],
      "id": "24ac0eb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N85q7TTqtXE"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    image = cv2.imread(path)\n",
        "    if image is not None:  # Check if image is loaded successfully\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        return image[...,::-1]\n",
        "    else:\n",
        "        print(f\"/content/drive/MyDrive/Colab Notebooks/train/fear: {path}\")\n",
        "        return None"
      ],
      "id": "3N85q7TTqtXE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiu6NfScq4Ng"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    image = cv2.imread(path)\n",
        "    if image is not None:  # Check if image is loaded successfully\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        return image[...,::-1]\n",
        "    else:\n",
        "        print(f\"/content/drive/MyDrive/Colab Notebooks/train/neutral: {path}\")\n",
        "        return None"
      ],
      "id": "hiu6NfScq4Ng"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD7yhyDsq8Ih"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    image = cv2.imread(path)\n",
        "    if image is not None:  # Check if image is loaded successfully\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        return image[...,::-1]\n",
        "    else:\n",
        "        print(f\"/content/drive/MyDrive/Colab Notebooks/train/surprise: {path}\")\n",
        "        return None"
      ],
      "id": "UD7yhyDsq8Ih"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80818963"
      },
      "source": [
        "# Modeling"
      ],
      "id": "80818963"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2b8eca6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/train\""
      ],
      "id": "d2b8eca6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77b2356f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "data_with_aug = ImageDataGenerator(horizontal_flip=True,\n",
        "                                   vertical_flip=False,\n",
        "                                   rescale=1./255,\n",
        "                                  validation_split=0.2)"
      ],
      "id": "77b2356f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b96690c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train = data_with_aug.flow_from_directory(dataset_path,\n",
        "                                          class_mode=\"binary\",\n",
        "                                          target_size=(96, 96),\n",
        "                                          batch_size=32,\n",
        "                                          subset=\"training\")"
      ],
      "id": "8b96690c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6693fdd5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "val = data_with_aug.flow_from_directory(dataset_path,\n",
        "                                          class_mode=\"binary\",\n",
        "                                          target_size=(96, 96),\n",
        "                                          batch_size=32,\n",
        "                                          subset=\"validation\"\n",
        "                                          )"
      ],
      "id": "6693fdd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "435cb698"
      },
      "source": [
        "# 1) MobileNetV2"
      ],
      "id": "435cb698"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a24f215e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "mnet = MobileNetV2(include_top = False, weights = \"imagenet\" ,input_shape=(96,96,3))"
      ],
      "id": "a24f215e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f00b0ad1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([mnet,\n",
        "                    GlobalAveragePooling2D(),\n",
        "                    Dense(512, activation = \"relu\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dropout(0.3),\n",
        "                    Dense(128, activation = \"relu\"),\n",
        "                    Dropout(0.1),\n",
        "                    # Dense(32, activation = \"relu\"),\n",
        "                    # Dropout(0.3),\n",
        "                    Dense(5, activation = \"sigmoid\")])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
        "\n",
        "model.summary()\n",
        "Model: \"sequential\""
      ],
      "id": "f00b0ad1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a74e6f37"
      },
      "outputs": [],
      "source": [
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 1:\n",
        "        return 0.001\n",
        "    elif epoch > 2 and epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "lr_callbacks = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "id": "a74e6f37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93fea36d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "hist = model.fit_generator(train,\n",
        "                    epochs=5,\n",
        "                    callbacks=[lr_callbacks],\n",
        "                    validation_data=val)"
      ],
      "id": "93fea36d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f69f6be"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "1f69f6be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6235728b"
      },
      "source": [
        "# 2) VGG16"
      ],
      "id": "6235728b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f3b191a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train = data_with_aug.flow_from_directory(dataset_path,\n",
        "                                          class_mode=\"binary\",\n",
        "                                          target_size=(224, 224),\n",
        "                                          batch_size=98,\n",
        "                                          subset=\"training\")\n",
        "\n",
        "val = data_with_aug.flow_from_directory(dataset_path,\n",
        "                                          class_mode=\"binary\",\n",
        "                                          target_size=(224, 224),\n",
        "                                          batch_size=98,\n",
        "                                          subset=\"validation\"\n",
        "                                          )"
      ],
      "id": "6f3b191a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45a7cc6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "vgg16_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))"
      ],
      "id": "45a7cc6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ce9e8f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "vgg16_model.output[-1]"
      ],
      "id": "83ce9e8f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af4e5783"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# model = Sequential()\n",
        "# for layer in vgg16_model.layers[:-1]:\n",
        "#     model.add(layer)\n",
        "\n",
        "# for layer in model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "model = Sequential([vgg16_model,\n",
        "                    Flatten(),\n",
        "#                     GlobalAveragePooling2D(),\n",
        "#                     Dense(512, activation = \"relu\"),\n",
        "#                     BatchNormalization(),\n",
        "#                     Dropout(0.3),\n",
        "#                     Dense(128, activation = \"relu\"),\n",
        "#                     Dropout(0.1),\n",
        "#                     # Dense(32, activation = \"relu\"),\n",
        "#                     # Dropout(0.3),\n",
        "                    Dense(5, activation = \"sigmoid\")])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
        "\n",
        "model.summary()"
      ],
      "id": "af4e5783"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5accd6b3",
        "outputId": "9b8a56ef-e794-43b2-b91b-182b8c4fdd0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-ea3365b6907f>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  hist =  model.fit_generator(train,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "77/77 [==============================] - 5202s 68s/step - loss: 0.8509 - accuracy: 0.6654 - val_loss: 0.6609 - val_accuracy: 0.7496 - lr: 0.0010\n",
            "Epoch 2/5\n",
            " 3/77 [>.............................] - ETA: 1:06:11 - loss: 0.5594 - accuracy: 0.8231"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "hist =  model.fit_generator(train,\n",
        "                    epochs=5,\n",
        "                    callbacks=[lr_callbacks],\n",
        "                    validation_data=val)"
      ],
      "id": "5accd6b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd36dcbc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "dd36dcbc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtNUFWFpt3Mo"
      },
      "outputs": [],
      "source": [],
      "id": "vtNUFWFpt3Mo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIXPqqdg3ix0"
      },
      "source": [
        "### 3)**InceptionV3**"
      ],
      "id": "EIXPqqdg3ix0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRytSBrWt3Cl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the InceptionV3 model pre-trained on ImageNet data\n",
        "inception_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    inception_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(5, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 2:\n",
        "        return 0.001\n",
        "    elif 2 < epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_callbacks = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Assuming you have already defined train and val data generators\n",
        "hist = model.fit(train, epochs=5, callbacks=[lr_callbacks], validation_data=val)"
      ],
      "id": "gRytSBrWt3Cl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wmJ17Fs6sCd"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "3wmJ17Fs6sCd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RJLdkQQ35mr"
      },
      "source": [
        "# **4)DenseNet201**"
      ],
      "id": "0RJLdkQQ35mr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPjMlaYFuLmk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the DenseNet201 model pre-trained on ImageNet data\n",
        "densenet201_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    densenet201_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(5, activation=\"softmax\")  # Adjust the number of units for your specific problem (multi-class classification)\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 2:\n",
        "        return 0.001\n",
        "    elif 2 < epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_callbacks = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Assuming you have already defined train and val data generators\n",
        "hist = model.fit(train, epochs=5, callbacks=[lr_callbacks], validation_data=val)"
      ],
      "id": "KPjMlaYFuLmk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9KPHrs16w7D"
      },
      "outputs": [],
      "source": [
        "epochs = 05\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "Q9KPHrs16w7D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMKLVPyx4aQB"
      },
      "source": [
        "# 5)**DenseNet169**"
      ],
      "id": "mMKLVPyx4aQB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLIa7csJvRXa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import DenseNet169\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the DenseNet169 model pre-trained on ImageNet data\n",
        "densenet169_model = DenseNet169(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    densenet169_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(5, activation=\"softmax\")  # Adjust the number of units for your specific problem (multi-class classification)\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 2:\n",
        "        return 0.001\n",
        "    elif 2 < epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_callbacks = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Assuming you have already defined train and val data generators\n",
        "hist = model.fit(train, epochs=5, callbacks=[lr_callbacks], validation_data=val)"
      ],
      "id": "YLIa7csJvRXa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "801Mb3aP60fR"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "801Mb3aP60fR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfbsfxmX4r2M"
      },
      "source": [
        "\n",
        "\n",
        "# 6)**VGG19**\n",
        "\n"
      ],
      "id": "ZfbsfxmX4r2M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddEopGO8vVRS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the VGG19 model pre-trained on ImageNet data\n",
        "vgg19_model = VGG19(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    vgg19_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(5, activation=\"softmax\")  # Adjust the number of units for your specific problem (multi-class classification)\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 2:\n",
        "        return 0.001\n",
        "    elif 2 < epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_callbacks = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Assuming you have already defined train and val data generators\n",
        "hist = model.fit(train, epochs=5, callbacks=[lr_callbacks], validation_data=val)"
      ],
      "id": "ddEopGO8vVRS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm0sDntN63ob"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "Fm0sDntN63ob"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi55vKCL46DQ"
      },
      "source": [
        "# 7)**ResNet152**"
      ],
      "id": "Qi55vKCL46DQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3v9Os6dvazx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the ResNet152 model pre-trained on ImageNet data\n",
        "resnet152_model = ResNet152(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    resnet152_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(4, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 2:\n",
        "        return 0.001\n",
        "    elif 2 < epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_callbacks = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Assuming you have already defined train and val data generators\n",
        "hist = model.fit(train, epochs=5, callbacks=[lr_callbacks], validation_data=val)"
      ],
      "id": "q3v9Os6dvazx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivm6XIFe69Xc"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "Ivm6XIFe69Xc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3RdRex35IE6"
      },
      "source": [
        "# 8)**ResNet101**"
      ],
      "id": "U3RdRex35IE6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whhAz9XOvgQJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the ResNet101 model pre-trained on ImageNet data\n",
        "resnet101_model = ResNet101(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    resnet101_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(5, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 2:\n",
        "        return 0.001\n",
        "    elif 2 < epoch <= 15:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_callbacks = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Assuming you have already defined train and val data generators\n",
        "hist = model.fit(train, epochs=5, callbacks=[lr_callbacks], validation_data=val)"
      ],
      "id": "whhAz9XOvgQJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_fj_N6avmqT"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "xc = range(epochs)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ],
      "id": "G_fj_N6avmqT"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3004.137011,
      "end_time": "2022-10-23T11:15:38.615746",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-10-23T10:25:34.478735",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}